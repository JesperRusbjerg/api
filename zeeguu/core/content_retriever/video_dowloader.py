import csv
import os
import zeeguu
import zeeguu.core.model
from zeeguu.core.constants import SIMPLE_TIME_FORMAT
from elasticsearch import Elasticsearch
from zeeguu.core.elastic.settings import ES_CONN_STRING, ES_ZINDEX
from sentry_sdk import capture_exception as capture_to_sentry
from zeeguu.core.elastic.converting_from_mysql import document_from_article
from zeeguu.core import log
from zeeguu.core.elastic.indexing import index_in_elasticsearch

session = zeeguu.core.db.session

TITLE = 0
DESCRIPTION = 1
URL = 2
CHANNEL = 3
PUBLISHED_AT = 4
PUBLISH_TIME = 5
QUERY = 6
CAPTIONS = 7
AUTOGENERATED = 8
TRANSLATED = 9
SRC_LANG = 10

# videoId,title,description,url,channelId,publishedAt,publishTime,query,captions,autogenerated,translated,src_lang


def get_videos_from_local_csv(filename):
    __location__ = os.path.realpath(
        os.path.join(os.getcwd(), os.path.dirname(__file__))
    )

    reader = csv.reader(open(os.path.join(__location__, filename)))

    result = {}

    for row in reader:
        key = row[0]
        if key in result:
            # implement your duplicate row handling here
            pass
        result[key] = row[1:]

    return result


def download(filename, fr, to):
    ai_videos = get_videos_from_local_csv(filename)

    for each in list(ai_videos.keys())[fr:to]:
        video_info = ai_videos[each]

        e = download_individual_video(video_info, core, model, video, index, document)


def download_individual_video(video_info, core, model, video, index, document):
    print("Processing: " + video_info[URL])

    from zeeguu.core.model import Article

    found = Article.find(video_info[URL])
    if found:
        print("FOUND ARTICLE ALREADY")
        # session.delete(found)
        # session.commit()
        return

    print("not found... creating it")
    from zeeguu.core.model import Url, Article, Language

    url_object = Url.find_or_create(session, video_info[URL])

    import datetime

    dt = datetime.datetime.strptime(video_info[PUBLISHED_AT], SIMPLE_TIME_FORMAT)
    dt = datetime.datetime.now()

    fr = Language.find("fr")

    new_article = Article(
        url_object,
        video_info[TITLE],
        "",
        video_info[CAPTIONS],  # any article longer than this will be truncated...
        video_info[DESCRIPTION],
        dt,
        None,
        fr,
        "",
        video=1,
    )
    session.add(new_article)
    session.commit()

    index_in_elasticsearch(new_article)


import sys

if __name__ == "__main__":

    print("run as main...")
    if len(sys.argv) > 3:
        download(sys.argv[1], int(sys.argv[2]), int(sys.argv[3]))
